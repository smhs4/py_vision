#!/usr/bin/env python3

import rospy
import numpy as np
import cv2
import open3d as o3d
import message_filters
import struct
import tf
from sensor_msgs.msg import Image, CameraInfo, PointCloud2, PointField
from geometry_msgs.msg import PoseStamped
import sensor_msgs.point_cloud2 as pc2
from cv_bridge import CvBridge
from ultralytics import YOLO
from scipy.spatial.transform import Rotation as R

class YoloHybridTracking:
    def __init__(self):
        rospy.init_node('yolo_hybrid_tracking')
        
        # 1. Configuration
        self.bridge = CvBridge()
        self.yolo = YOLO('src/best.pt') 
        self.step = 3
        self.target_frame = "base_link" 
        
        # --- TUNING PARAMETERS ---
        # 1. Geometric Offset: How deep is the center from the surface?
        self.cup_radius = 0.04  # 4cm
        
        # 2. Trimmed ICP: Max distance to look for a match.
        # Set this SMALL (e.g., 1.5cm). 
        # Points on the back of the CAD model (> 4cm away) will be ignored.
        self.trim_threshold = 0.015 
        
        self.tf_listener = tf.TransformListener()
        self.fx, self.fy, self.cx, self.cy = 600.0, 600.0, 320.0, 240.0
        
        # 2. State
        self.last_pose_camera_frame = np.identity(4)
        self.is_tracking = False

        # 3. Load CAD
        try:
            mesh = o3d.io.read_triangle_mesh("src/cup.stl")
            self.source_pcd = mesh.sample_points_uniformly(number_of_points=1000)
            self.source_center = self.source_pcd.get_center()
            self.source_pcd.translate(-self.source_center)
            
            # Compute Normals (Required for Point-to-Plane)
            self.source_pcd.estimate_normals(
                search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.03, max_nn=30)
            )
            rospy.loginfo("Node Ready: Hybrid Tracking (Offset + Trimmed ICP)")
        except Exception as e:
            rospy.logerr(f"Failed to load STL: {e}")
            return

        # 4. Publishers
        self.pose_pub = rospy.Publisher('/object_pose', PoseStamped, queue_size=1)
        self.pcl_pub = rospy.Publisher('/yolo/cleaned_cloud', PointCloud2, queue_size=1)
        self.debug_pub = rospy.Publisher('/debug/yolo_box', Image, queue_size=1)

        # 5. Subscribers
        self.info_sub = rospy.Subscriber('/sciurus17/camera/color/camera_info', CameraInfo, self.info_callback)
        rgb_sub = message_filters.Subscriber('/sciurus17/camera/color/image_raw', Image)
        depth_sub = message_filters.Subscriber('/sciurus17/camera/aligned_depth_to_color/image_raw', Image)
        self.ts = message_filters.ApproximateTimeSynchronizer([rgb_sub, depth_sub], 10, 0.1)
        self.ts.registerCallback(self.callback)

    def info_callback(self, msg):
        k = np.array(msg.K).reshape(3,3)
        self.fx, self.fy, self.cx, self.cy = k[0,0], k[1,1], k[0,2], k[1,2]
        self.info_sub.unregister() 

    def remove_farthest_plane(self, pcd):
        points = np.asarray(pcd.points)
        if len(points) < 50: return pcd

        z_coords = points[:, 2]
        median_z = np.median(z_coords)
        far_indices = np.where(z_coords > median_z)[0]
        
        if len(far_indices) < 20: return pcd
        
        # Only fit plane to the farthest points (background)
        pcd_far = pcd.select_by_index(far_indices)
        plane_model, _ = pcd_far.segment_plane(distance_threshold=0.02, ransac_n=3, num_iterations=500)
        
        [a, b, c, d] = plane_model
        distances = np.abs(np.dot(points, np.array([a, b, c])) + d)
        
        # Remove anything close to that plane
        return pcd.select_by_index(np.where(distances > 0.02)[0])

    def estimate_pose_hybrid(self, target_pcd):
        """
        Combines Geometric Offset (Guess) + Trimmed ICP (Refinement)
        """
        # 1. Surface Centroid
        surface_center = target_pcd.get_center()
        
        # 2. Compute Target Normals
        target_pcd.estimate_normals(
            search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.03, max_nn=30)
        )

        # 3. Build Initial Guess (Geometric Offset)
        T_guess = np.identity(4)
        
        # Calculate the "Deep Center" (Surface + Radius along Z)
        deep_center = surface_center.copy()
        deep_center[2] += self.cup_radius 
        
        if self.is_tracking:
            # Use Memory for Rotation
            T_guess[:3, :3] = self.last_pose_camera_frame[:3, :3]
            T_guess[:3, 3] = deep_center # Snap to deep center
        else:
            # First time: Identity Rotation
            T_guess[:3, 3] = deep_center

        # 4. Run Trimmed Point-to-Plane ICP
        reg_p2l = o3d.pipelines.registration.registration_icp(
            self.source_pcd, target_pcd, 
            
            # --- THE TRIM ---
            # We strictly enforce that correspondences must be within 1.5cm.
            # Points on the back of the cup (which are 8cm away) get ignored.
            self.trim_threshold, 
            
            T_guess,
            o3d.pipelines.registration.TransformationEstimationPointToPlane(),
            o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=30)
        )
        
        # 5. Check Fitness
        # Note: Fitness will be lower because we are intentionally ignoring half the model.
        # A fitness of 0.3 (30%) is actually quite good here.
        if reg_p2l.fitness < 0.2: 
            self.is_tracking = False
            return reg_p2l.transformation
        
        self.last_pose_camera_frame = reg_p2l.transformation
        self.is_tracking = True
        
        return reg_p2l.transformation

    def publish_cloud(self, pcd, header):
        points = np.asarray(pcd.points)
        colors = np.asarray(pcd.colors)
        if len(points) == 0: return

        r = (colors[:, 0] * 255).astype(np.uint32)
        g = (colors[:, 1] * 255).astype(np.uint32)
        b = (colors[:, 2] * 255).astype(np.uint32)
        
        rgb_int = (r << 16) | (g << 8) | (b)
        rgb_float = np.array([struct.unpack('f', struct.pack('I', i))[0] for i in rgb_int])
        
        points_3d = np.column_stack((points[:, 0], points[:, 1], points[:, 2], rgb_float))

        fields = [
            PointField('x', 0, PointField.FLOAT32, 1),
            PointField('y', 4, PointField.FLOAT32, 1),
            PointField('z', 8, PointField.FLOAT32, 1),
            PointField('rgb', 12, PointField.FLOAT32, 1),
        ]
        pc_msg = pc2.create_cloud(header, fields, points_3d)
        self.pcl_pub.publish(pc_msg)

    def callback(self, rgb_msg, depth_msg):
        try:
            cv_rgb = self.bridge.imgmsg_to_cv2(rgb_msg, "bgr8")
            cv_depth = self.bridge.imgmsg_to_cv2(depth_msg, "16UC1")
        except Exception: return

        results = self.yolo(cv_rgb, classes=[0], verbose=False, conf=0.4)
        if not results or not results[0].boxes: 
            self.is_tracking = False 
            return
            
        box = results[0].boxes.xyxy[0].cpu().numpy().astype(int)
        
        cv2.rectangle(cv_rgb, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)
        self.debug_pub.publish(self.bridge.cv2_to_imgmsg(cv_rgb, "bgr8"))

        h, w = cv_depth.shape
        x1, y1 = max(0, box[0]), max(0, box[1])
        x2, y2 = min(w, box[2]), min(h, box[3])

        rgb_roi = cv_rgb[y1:y2:self.step, x1:x2:self.step]
        depth_roi = cv_depth[y1:y2:self.step, x1:x2:self.step]
        if depth_roi.size == 0: return

        h_roi, w_roi = depth_roi.shape
        u_col = np.arange(0, w_roi) * self.step + x1
        v_row = np.arange(0, h_roi) * self.step + y1
        u_map, v_map = np.meshgrid(u_col, v_row)
        z = depth_roi.astype(np.float32) / 1000.0 
        
        valid_mask = (z > 0.1) & (z < 2.0)
        if np.count_nonzero(valid_mask) < 20: return
        
        x = (u_map[valid_mask] - self.cx) * z[valid_mask] / self.fx
        y = (v_map[valid_mask] - self.cy) * z[valid_mask] / self.fy
        z_valid = z[valid_mask]
        
        points = np.stack((x, y, z_valid), axis=-1)
        
        b = rgb_roi[:, :, 0][valid_mask]
        g = rgb_roi[:, :, 1][valid_mask]
        r = rgb_roi[:, :, 2][valid_mask]
        colors = np.stack((r, g, b), axis=-1) / 255.0

        target_pcd = o3d.geometry.PointCloud()
        target_pcd.points = o3d.utility.Vector3dVector(points)
        target_pcd.colors = o3d.utility.Vector3dVector(colors)

        target_pcd = self.remove_farthest_plane(target_pcd)
        if len(target_pcd.points) < 10: return
        
        self.publish_cloud(target_pcd, rgb_msg.header)

        # ESTIMATE WITH HYBRID LOGIC
        final_transform = self.estimate_pose_hybrid(target_pcd)
        self.publish_ros_pose(final_transform, rgb_msg.header)

    def publish_ros_pose(self, camera_T_object, header):
        try:
            (trans, rot) = self.tf_listener.lookupTransform(
                self.target_frame, header.frame_id, rospy.Time(0)
            )
            
            t_mat = tf.transformations.translation_matrix(trans)
            r_mat = tf.transformations.quaternion_matrix(rot)
            base_T_camera = np.dot(t_mat, r_mat)
            
            base_T_object = np.dot(base_T_camera, camera_T_object)
            
            p = PoseStamped()
            p.header.stamp = rospy.Time.now()
            p.header.frame_id = self.target_frame
            
            p.pose.position.x = base_T_object[0, 3]
            p.pose.position.y = base_T_object[1, 3]
            p.pose.position.z = base_T_object[2, 3]
            
            r = R.from_matrix(base_T_object[:3, :3].copy()) 
            quat = r.as_quat() 
            p.pose.orientation.x = quat[0]
            p.pose.orientation.y = quat[1]
            p.pose.orientation.z = quat[2]
            p.pose.orientation.w = quat[3]
            
            self.pose_pub.publish(p)

        except Exception as e:
            rospy.logwarn_throttle(2, f"TF Error: {e}")

if __name__ == '__main__':
    try:
        YoloHybridTracking()
        rospy.spin()
    except rospy.ROSInterruptException:
        pass