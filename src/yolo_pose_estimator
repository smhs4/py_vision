#!/usr/bin/env python3

import rospy
import numpy as np
import cv2
import open3d as o3d
import message_filters
import struct
import tf # <--- REQUIRED FOR COORDINATE TRANSFORMS
from sensor_msgs.msg import Image, CameraInfo, PointCloud2, PointField
from geometry_msgs.msg import PoseStamped
import sensor_msgs.point_cloud2 as pc2
from cv_bridge import CvBridge
from ultralytics import YOLO
from scipy.spatial.transform import Rotation as R

class YoloPoseTransformer:
    def __init__(self):
        rospy.init_node('yolo_pose_transformer')
        
        # 1. Configuration
        self.bridge = CvBridge()
        self.yolo = YOLO('src/best.pt') 
        self.step = 2
        self.plane_threshold = 0.001
        
        # MEMORY: Initialize previous rotation as Identity (or None)
        self.last_R = np.eye(3) 
        self.is_initialized = False # Flag to know if we have a valid history
        
        # TARGET FRAME: The coordinate system you want the output in
        self.target_frame = "base_link" 
        
        # TF Listener
        self.tf_listener = tf.TransformListener()
        
        self.fx, self.fy, self.cx, self.cy = 600.0, 600.0, 320.0, 240.0
        
        # 2. Load CAD Model (Source)
        try:
            mesh = o3d.io.read_triangle_mesh("src/cup.stl")
            mesh.scale(2.0, center=mesh.get_center())
            self.source_pcd = mesh.sample_points_uniformly(number_of_points=3000)
            self.source_pcd.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.005, max_nn=30))
            self.source_center = self.source_pcd.get_center()
            self.source_pcd.translate(-self.source_center)
            rospy.loginfo(f"Loaded CAD Model: {len(self.source_pcd.points)} points")
        except Exception as e:
            rospy.logerr(f"Failed to load STL: {e}")
            return

        # 3. Publishers
        self.pose_pub = rospy.Publisher('/object_pose', PoseStamped, queue_size=1)
        self.pcl_pub = rospy.Publisher('/yolo/cleaned_cloud', PointCloud2, queue_size=1)
        self.debug_pub = rospy.Publisher('/debug/yolo_box', Image, queue_size=1)

        # 4. Subscribers
        self.info_sub = rospy.Subscriber(
            '/sciurus17/camera/color/camera_info', 
            CameraInfo, self.info_callback
        )
        
        rgb_sub = message_filters.Subscriber('/sciurus17/camera/color/image_raw', Image)
        depth_sub = message_filters.Subscriber('/sciurus17/camera/aligned_depth_to_color/image_raw', Image)
        
        self.ts = message_filters.ApproximateTimeSynchronizer([rgb_sub, depth_sub], 10, 0.1)
        self.ts.registerCallback(self.callback)
        
        rospy.loginfo("Node Ready: Outputting in 'base_link' frame.")

    def info_callback(self, msg):
        k = np.array(msg.K).reshape(3,3)
        self.fx, self.fy, self.cx, self.cy = k[0,0], k[1,1], k[0,2], k[1,2]
        self.info_sub.unregister() 

    def remove_farthest_plane(self, pcd):
        points = np.asarray(pcd.points)
        if len(points) < 50: return pcd

        z_coords = points[:, 2]
        median_z = np.median(z_coords)
        far_indices = np.where(z_coords > median_z)[0]
        
        if len(far_indices) < 20: return pcd

        pcd_far = pcd.select_by_index(far_indices)
        plane_model, inliers = pcd_far.segment_plane(distance_threshold=self.plane_threshold,
                                                     ransac_n=3, num_iterations=500)
        # Use the inliers themselves
        # [a, b, c, d] = plane_model
        # distances = np.abs(np.dot(points, np.array([a, b, c])) + d)
        # keep_mask = distances > self.plane_threshold
        
        # return pcd.select_by_index(np.where(keep_mask)[0])

        return pcd.select_by_index(inliers, invert=True)

    def DBSCAN(self, pcd):
        labels = np.array(pcd.cluster_dbscan(eps=0.1, min_points=20))
        valid = labels[labels >= 0]
        counts = np.bincount(valid)

        top = counts.argmax()
        idx = np.where(labels == top)[0]
        pcd_keep = pcd.select_by_index(idx)

        return pcd_keep

    def color_filter(self, pcd):
        pcd_colors = np.asarray(pcd.colors)
        min_levels = np.array([0, 0, 0])
        max_levels = np.array([0.5, 0.5, 0.5])

        mask = np.all((pcd_colors >= min_levels) & (pcd_colors <= max_levels), axis=1)
        indicies = np.where(mask)[0]
        return pcd.select_by_index(indicies)
    

    def estimate_pose_pca_icp_nn(self, target_pcd):
        # --- 1. PRE-PROCESSING & CENTERING ---
        target_center = target_pcd.get_center() + np.array([0, 0, 0.05])
        
        # --- CALCULATE NORMALS FOR TARGET (NEW) ---
        # 1. Estimate normals based on local neighbors
        # Radius=0.03 (3cm) is usually a good starting point for manipulation objects
        target_pcd.estimate_normals(
            search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.005, max_nn=30)
        )
        
        # 2. Align normals towards the camera (0,0,0)
        # This fixes the "inside-out" problem common with depth cameras
        target_pcd.orient_normals_towards_camera_location(camera_location=np.array([0., 0., 0.]))

        # --- 2. POINT NUMBER MATCHING ---
        n_target = len(target_pcd.points)
        n_source = len(self.source_pcd.points)
        
        # if n_source > n_target:
        #     ratio = n_target / n_source
        #     source_used = self.source_pcd.random_down_sample(ratio)
        # else:
        #     source_used = self.source_pcd
        source_used = self.source_pcd

        # --- 3. INITIALIZATION STRATEGY ---
        candidates = [] 

        # STRATEGY A: MEMORY
        if self.is_initialized:
            T_mem = np.eye(4)
            T_mem[:3, :3] = self.last_R
            T_mem[:3, 3] = target_center
            candidates.append((T_mem, "Memory"))

        # STRATEGY B: 6 CUBE FACES
        rot_1 = np.eye(3)                                    # Identity
        rot_2 = np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]]) # Upside down
        rot_3 = np.array([[1, 0, 0], [0, 0, -1], [0, 1, 0]])  # Up
        rot_4 = np.array([[1, 0, 0], [0, 0, 1], [0, -1, 0]])  # Down
        rot_5 = np.array([[0, 0, 1], [0, 1, 0], [-1, 0, 0]])  # Right
        rot_6 = np.array([[0, 0, -1], [0, 1, 0], [1, 0, 0]])  # Left

        cube_rotations = [rot_1, rot_2, rot_3, rot_4, rot_5, rot_6]

        for R_guess in cube_rotations:
            T_guess = np.eye(4)
            T_guess[:3, :3] = R_guess
            T_guess[:3, 3] = target_center
            candidates.append((T_guess, "Search"))

        # --- 4. COARSE SEARCH (Quick ICP) ---
        best_fitness = -1.0
        best_T = np.eye(4)
        
        # CHANGED: Use PointToPlane
        estimation_method = o3d.pipelines.registration.TransformationEstimationPointToPlane()
        
        # Use a looser convergence for the coarse search
        criteria_fast = o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=10)

        for T_init, strategy in candidates:
            try:
                reg_quick = o3d.pipelines.registration.registration_icp(
                    source_used, target_pcd, 
                    0.005, 
                    T_init,
                    estimation_method,
                    criteria_fast
                )
                
                if strategy == "Memory" and not reg_quick.fitness < 0.1:
                    best_T = reg_quick.transformation
                    best_fitness = reg_quick.fitness
                    rospy.loginfo(f"Using memory: {reg_quick.fitness}")
                    break
                    
                if reg_quick.fitness > best_fitness:
                    best_fitness = reg_quick.fitness
                    best_T = reg_quick.transformation
                    rospy.loginfo(f"No memory: {reg_quick.fitness}")
            except RuntimeError:
                # Point-to-Plane can fail if normals are bad or points don't overlap. 
                # We catch this to prevent crashing the loop.
                continue

        # --- 5. FINE REFINEMENT (Final ICP) ---
        reg_final = o3d.pipelines.registration.registration_icp(
            source_used, target_pcd, 
            0.005, 
            best_T,
            estimation_method,
            o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=50)
        )

        # --- 6. UPDATE MEMORY ---
        self.last_R = reg_final.transformation[:3, :3]
        self.is_initialized = True 

        return reg_final.transformation


    def estimate_pose_pca_icp(self, target_pcd):
        # --- 1. PRE-PROCESSING & CENTERING ---
        # Apply your specific offset for the shell
        target_center = target_pcd.get_center() + np.array([0, 0, 0.05])
        
        points = np.asarray(target_pcd.points)
        n_target = len(points)
        rospy.loginfo_once(f"Target points: {n_target}")

        # --- 2. POINT NUMBER MATCHING ---
        # n_source = len(self.source_pcd.points)
        # if n_source > n_target:
        #     # Calculate ratio needed to get roughly n_target points
        #     ratio = n_target / n_source
        #     source_used = self.source_pcd.random_down_sample(ratio)
        # else:
        #     source_used = self.source_pcd
        source_used = self.source_pcd

        # --- 3. INITIALIZATION STRATEGY ---
        candidates = [] 

        # STRATEGY A: MEMORY (Use last known rotation + NEW center)
        if self.is_initialized:
            T_mem = np.eye(4)
            T_mem[:3, :3] = self.last_R
            T_mem[:3, 3] = target_center
            candidates.append((T_mem, "Memory"))

        # STRATEGY B: 6 CUBE FACES (Cardinal Axes)
        # These 6 matrices cover the object resting on any of its 6 faces.
        
        # 1. Identity (0 deg)
        rot_1 = np.eye(3)
        
        # 2. 180 deg around X (Upside down)
        rot_2 = np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]])
        
        # 3. 90 deg around X (Facing Up)
        rot_3 = np.array([[1, 0, 0], [0, 0, -1], [0, 1, 0]])
        
        # 4. -90 deg around X (Facing Down)
        rot_4 = np.array([[1, 0, 0], [0, 0, 1], [0, -1, 0]])
        
        # 5. 90 deg around Y (Facing Right)
        rot_5 = np.array([[0, 0, 1], [0, 1, 0], [-1, 0, 0]])
        
        # 6. -90 deg around Y (Facing Left)
        rot_6 = np.array([[0, 0, -1], [0, 1, 0], [1, 0, 0]])

        cube_rotations = [rot_1, rot_2, rot_3, rot_4, rot_5, rot_6]

        for R_guess in cube_rotations:
            T_guess = np.eye(4)
            T_guess[:3, :3] = R_guess
            T_guess[:3, 3] = target_center
            candidates.append((T_guess, "Search"))

        # --- 4. COARSE SEARCH (Quick ICP) ---
        best_fitness = -1.0
        best_T = np.eye(4)
        
        estimation_method = o3d.pipelines.registration.TransformationEstimationPointToPoint()
        criteria_fast = o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=10)

        for T_init, strategy in candidates:
            reg_quick = o3d.pipelines.registration.registration_icp(
                source_used, target_pcd, 
                0.01, 
                T_init,
                estimation_method,
                criteria_fast
            )
            
            # If Memory is excellent (e.g., > 90% overlap), break early to save computation
            if strategy == "Memory" and not (reg_quick.fitness < 0.2):
                best_T = reg_quick.transformation
                best_fitness = reg_quick.fitness
                rospy.loginfo(f"Using memory: {reg_quick.fitness}")
                break
                
            if reg_quick.fitness > best_fitness:
                best_fitness = reg_quick.fitness
                best_T = reg_quick.transformation
                rospy.loginfo(f"No memory: {reg_quick.fitness}")

        # --- 5. FINE REFINEMENT (Final ICP) ---
        # Now run a high-quality registration on the winner
        reg_final = o3d.pipelines.registration.registration_icp(
            source_used, target_pcd, 
            0.001, 
            best_T,
            estimation_method,
            o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=50)
        )

        # --- 6. UPDATE MEMORY ---
        self.last_R = reg_final.transformation[:3, :3]
        self.is_initialized = True 

        return reg_final.transformation

    def publish_cloud(self, pcd, header):
        points = np.asarray(pcd.points)
        colors = np.asarray(pcd.colors)
        if len(points) == 0: return

        r = (colors[:, 0] * 255).astype(np.uint32)
        g = (colors[:, 1] * 255).astype(np.uint32)
        b = (colors[:, 2] * 255).astype(np.uint32)
        
        rgb_int = (r << 16) | (g << 8) | (b)
        rgb_float = np.array([struct.unpack('f', struct.pack('I', i))[0] for i in rgb_int])
        
        points_3d = np.column_stack((points[:, 0], points[:, 1], points[:, 2], rgb_float))

        fields = [
            PointField('x', 0, PointField.FLOAT32, 1),
            PointField('y', 4, PointField.FLOAT32, 1),
            PointField('z', 8, PointField.FLOAT32, 1),
            PointField('rgb', 12, PointField.FLOAT32, 1),
        ]

        pc_msg = pc2.create_cloud(header, fields, points_3d)
        self.pcl_pub.publish(pc_msg)

    def callback(self, rgb_msg, depth_msg):
        try:
            cv_rgb = self.bridge.imgmsg_to_cv2(rgb_msg, "bgr8")
            cv_depth = self.bridge.imgmsg_to_cv2(depth_msg, "16UC1")
        except Exception: return

        results = self.yolo(cv_rgb, classes=[0], verbose=False, conf=0.4)
        if not results or not results[0].boxes: return
        box = results[0].boxes.xyxy[0].cpu().numpy().astype(int)
        
        # cv2.rectangle(cv_rgb, (box[0], box[1]), (box[2], box[3]), (0, 255, 0), 2)
        # self.debug_pub.publish(self.bridge.cv2_to_imgmsg(cv_rgb, "bgr8"))

        h, w = cv_depth.shape
        x1, y1 = max(0, box[0]), max(0, box[1])
        x2, y2 = min(w, box[2]), min(h, box[3])

        rgb_roi = cv_rgb[y1:y2:self.step, x1:x2:self.step]
        depth_roi = cv_depth[y1:y2:self.step, x1:x2:self.step]
        if depth_roi.size == 0: return

        h_roi, w_roi = depth_roi.shape
        u_col = np.arange(0, w_roi) * self.step + x1
        v_row = np.arange(0, h_roi) * self.step + y1
        u_map, v_map = np.meshgrid(u_col, v_row)
        z = depth_roi.astype(np.float32) / 1000.0 
        
        valid_mask = (z > 0.1) & (z < 2.0)
        if np.count_nonzero(valid_mask) < 20: return
        
        x = (u_map[valid_mask] - self.cx) * z[valid_mask] / self.fx
        y = (v_map[valid_mask] - self.cy) * z[valid_mask] / self.fy
        z_valid = z[valid_mask]
        
        points = np.stack((x, y, z_valid), axis=-1)
        
        b = rgb_roi[:, :, 0][valid_mask]
        g = rgb_roi[:, :, 1][valid_mask]
        r = rgb_roi[:, :, 2][valid_mask]
        colors = np.stack((r, g, b), axis=-1) / 255.0

        target_pcd = o3d.geometry.PointCloud()
        target_pcd.points = o3d.utility.Vector3dVector(points)
        target_pcd.colors = o3d.utility.Vector3dVector(colors)

        rospy.loginfo_once(colors[100][0])
        rospy.loginfo_once(colors[100][1])
        rospy.loginfo_once(colors[100][2])

        rospy.loginfo_once(colors[200][0])
        rospy.loginfo_once(colors[200][1])
        rospy.loginfo_once(colors[200][2])

        # target_pcd = self.remove_farthest_plane(target_pcd)
        # if len(target_pcd.points) < 10: return
        target_pcd = self.color_filter(target_pcd)
        # target_pcd = self.DBSCAN(target_pcd)
        # if len(target_pcd.points) < 10: return
        
        self.publish_cloud(target_pcd, rgb_msg.header)

        # final_transform = self.estimate_pose_pca_icp(target_pcd)
        final_transform = self.estimate_pose_pca_icp_nn(target_pcd)
        self.publish_ros_pose(final_transform, rgb_msg.header)

    def publish_ros_pose(self, camera_T_object, header):
        """
        TRANSFORMS: Camera Frame -> Base Frame
        """
        try:
            # 1. Lookup Transform: Base -> Camera (At the time the image was taken)
            # This returns (translation, rotation)
            (trans, rot) = self.tf_listener.lookupTransform(
                self.target_frame, header.frame_id, rospy.Time(0)
            )
            
            # 2. Convert ROS transform to 4x4 Numpy Matrix
            # base_T_camera
            t_mat = tf.transformations.translation_matrix(trans)
            r_mat = tf.transformations.quaternion_matrix(rot)
            base_T_camera = np.dot(t_mat, r_mat)
            
            # 3. Multiply Matrices
            # base_T_object = base_T_camera * camera_T_object
            base_T_object = np.dot(base_T_camera, camera_T_object)
            
            # 4. Extract for Message
            p = PoseStamped()
            p.header.stamp = rospy.Time.now()
            p.header.frame_id = self.target_frame # Now it is 'base_link'
            
            p.pose.position.x = base_T_object[0, 3]
            p.pose.position.y = base_T_object[1, 3]
            p.pose.position.z = base_T_object[2, 3]
            
            r = R.from_matrix(base_T_object[:3, :3].copy()) 
            quat = r.as_quat() 
            p.pose.orientation.x = quat[0]
            p.pose.orientation.y = quat[1]
            p.pose.orientation.z = quat[2]
            p.pose.orientation.w = quat[3]
            
            self.pose_pub.publish(p)

        except (tf.LookupException, tf.ConnectivityException, tf.ExtrapolationException) as e:
            rospy.logwarn_throttle(2, f"TF Error: {e}")

if __name__ == '__main__':
    try:
        YoloPoseTransformer()
        rospy.spin()
    except rospy.ROSInterruptException:
        pass